<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


    <title>Weicheng Dai</title>
    
    <meta name="author" content="Weicheng Dai">
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>


<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:0px">
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr style="padding:0px">
                        <td style="padding:2.5%;width:63%;vertical-align:middle">
                            <p style="text-align:center">
                                <name>Weicheng Dai</name>
                            </p>
                            <p>I am currently a phd student at Boston University, where I am very fortunately advised by Professor <a href="https://batman-lab.com/">Kayhan Batmanghelich</a>. Previously I worked at Yale University for Professor <a href="https://medicine.yale.edu/profile/julius-chapiro/">Julius Chapiro</a> and Professor <a href="https://seas.yale.edu/faculty-research/faculty-directory/james-duncan">James S. Duncan</a>. I also worked with Professor <a href="http://chenyuyou.me/">Chenyu You</a>. I obtained a Master degree in Computer Science at New York University. My research interests include computer & medical vision, and explainable machine learning.
                            </p>
                            <p style="text-align:center">
                                <a href="mailto:daiweicheng4536@gmail.com">Email</a> &nbsp/&nbsp
                                <a href="https://www.linkedin.com/in/weicheng-dai-2926851bb/">LinkedIn</a> &nbsp/&nbsp
                                <a href="https://github.com/WeichengDai1/">Github</a>
                            </p>
                        </td>
                        <td style="padding:2.5%;width:40%;max-width:40%">
                            <a href="images/self.jpg">
                                <img style="width:100%;max-width:100%" alt="profile photo" src="images/self.jpg" class="hoverZoomLink"></a>
                        </td>
                    </tr>
                </tbody></table>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                    <heading>Research</heading>
                    </td>
                </tr>
                </tbody></table>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                
                <tr onmouseout="mira_stop()" onmouseover="mira_start()">
                    <td style="padding:60px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img src='images/action.jpg' width="260">
                    </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2206.02307">
                        <papertitle>Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation</papertitle>
                    </a>
                    <br>
                    <a href="http://chenyuyou.me/">Chenyu You</a>, <strong>Weicheng Dai</strong>, 
                    <a href="https://www.linkedin.com/in/yifei-min-21957254/">Yifei Min</a>,
                    <a href="https://medicine.yale.edu/profile/lawrence-staib/">Lawrence Staib</a>,
                    <a href="https://seas.yale.edu/faculty-research/faculty-directory/james-duncan">James S. Duncan</a>,
                    <br>
                    <em>Accepted by Information Processing in Medical Imaging (IPMI 2023)</em>
                    <p></p>
                    <p>
                        In this work, we present ACTION, an Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised medical image segmentation. 
                        
                    </p>
                    </td>
                </tr>	

                <tr onmouseout="mira_stop()" onmouseover="mira_start()">
                    <td style="padding:60px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img src='images/mona.jpg' width="260">
                    </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2209.13476">
                        <papertitle>Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels</papertitle>
                    </a>
                    <br>
                    <a href="http://chenyuyou.me/">Chenyu You*</a>, <strong>Weicheng Dai*</strong>, 
                    <a href="https://www.linkedin.com/in/yifei-min-21957254/">Yifei Min</a>,
                    <a href="https://fenglinliu.com/">Fenglin Liu</a>,
                    <a href="https://xxlya.github.io/xiaoxiao/">Xiaoxiao Li</a>,
                    <a href="https://ibme.ox.ac.uk/people/david-clifton/">David A. Clifton</a>,
                    <a href="https://medicine.yale.edu/profile/lawrence-staib/">Lawrence Staib</a>,
                    <a href="https://seas.yale.edu/faculty-research/faculty-directory/james-duncan">James S. Duncan</a>,
                    <br>
                    <em>Accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI 2023)</em>
                    <p></p>
                    <p>
                        (* denotes equal contribution) Three simple principles: (1) tailness: giving more importance to tail class hard pixels; (2) consistency: enforcing the feature invariances to specified data transformations; (3) diversity: ensuring anatomical diversity in the set of different sampled images in those imbalanced, unlabeled, and diverse scenarios.
                    </p>
                    </td>
                </tr>	

                <tr onmouseout="mira_stop()" onmouseover="mira_start()">
                    <td style="padding:60px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img src='images/stratified.png' width="260">
                    </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2302.01735">
                        <papertitle>Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective</papertitle>
                    </a>
                    <br>
                    <a href="http://chenyuyou.me/">Chenyu You</a>, <strong>Weicheng Dai</strong>, 
                    <a href="https://www.linkedin.com/in/yifei-min-21957254/">Yifei Min</a>
                    <a href="https://fenglinliu.com/">Fenglin Liu</a>,
                    <a href="https://xiaoranzhang.com/">Xiaoran Zhang</a>,
                    <a href="https://ibme.ox.ac.uk/people/david-clifton/">David A. Clifton</a>,
                    <a href="https://www.linkedin.com/in/s-kevin-zhou-231a094b/">S Kevin Zhou</a>,
                    <a href="https://medicine.yale.edu/profile/lawrence-staib/">Lawrence Staib</a>,
                    <a href="https://seas.yale.edu/faculty-research/faculty-directory/james-duncan">James S. Duncan</a>,
                    <br>
                    <em>Accepted by Conference on Neural Information Processing Systems (NeurIPS 2023)</em>
                    <p></p>
                    <p>
                        Two practical solutions via stratified group sampling theory that correct for the variance introduced by the common sampling practice, and achieve significant performance benefits. </p>
                    </td>
                </tr>


                <tr onmouseout="mira_stop()" onmouseover="mira_start()">
                    <td style="padding:60px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img src='images/action++.png' width="260">
                    </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="http://arxiv.org/abs/2304.02689">
                        <papertitle>ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast</papertitle>
                    </a>
                    <br>
                    <a href="http://chenyuyou.me/">Chenyu You</a>, <strong>Weicheng Dai</strong>, 
                    <a href="https://www.linkedin.com/in/yifei-min-21957254/">Yifei Min</a>
                    <a href="https://medicine.yale.edu/profile/lawrence-staib/">Lawrence Staib</a>,
                    <a href="https://statistics.yale.edu/people/jas-sekhon">Jasjeet S. Sekhon</a>,
                    <a href="https://seas.yale.edu/faculty-research/faculty-directory/james-duncan">James S. Duncan</a>,
                    <br>
                    <em>Early accepted by International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2023)</em>
                    <p></p>
                    <p>
                        In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. We propose an adaptive supervised
                        contrastive loss, where we compute the optimal locations of class
                        centers uniformly distributed on the embedding space. We also use dynamic Tau to yield better separation between majority and minority classe. </p>
                    </td>
                </tr>

                <tr onmouseout="mira_stop()" onmouseover="mira_start()">
                    <td style="padding:60px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img src='images/implicit.png' width="260">
                    </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="http://arxiv.org/abs/2304.03209">
                        <papertitle>Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts</papertitle>
                    </a>
                    <br>
                    <a href="http://chenyuyou.me/">Chenyu You</a>, <strong>Weicheng Dai</strong>, 
                    <a href="https://www.linkedin.com/in/yifei-min-21957254/">Yifei Min</a>
                    <a href="https://medicine.yale.edu/profile/lawrence-staib/">Lawrence Staib</a>,
                    <a href="https://seas.yale.edu/faculty-research/faculty-directory/james-duncan">James S. Duncan</a>,
                    <br>
                    <em>Early accepted by International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2023)</em>
                    <p></p>
                    <p>
                        In this work, we propose MORSE, a generic implicit neural rendering framework designed at an anatomical level to assist learning in medical image segmentation. 
                        The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. </p>
                    </td>
                </tr>

                <tr onmouseout="mira_stop()" onmouseover="mira_start()">
                    <td style="padding:60px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img src='images/CFR.png' width="260">
                    </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2403.07241">
                        <papertitle>Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations</papertitle>
                    </a>
                    <br>
                    <a href="http://chenyuyou.me/">Chenyu You</a>,  
                    <a href="https://www.linkedin.com/in/yifei-min-21957254/">Yifei Min</a>,
                    <strong>Weicheng Dai</strong>,
                    <a href="https://statistics.yale.edu/people/jas-sekhon">Jasjeet S. Sekhon</a>,
                    <a href="https://medicine.yale.edu/profile/lawrence-staib/">Lawrence Staib</a>,
                    <a href="https://seas.yale.edu/faculty-research/faculty-directory/james-duncan">James S. Duncan</a>,
                    <br>
                    <em>Accepted by The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR 2024)</em>
                    <p></p>
                    <p>
                        In this work, we propose CFR, which focuses on exploring mitigating the reliance on spurious features for CLIP without using any group annotation. 
                        We showcase a lightweight representation calibration method for fine-tuning CLIP, by first generating a calibration set using the pretrained CLIP, and then calibrating representations of samples within this set through contrastive learning.</p>
                    </td>
                </tr>

            </td>
        </tr>


    </tbody>
    </table>

</body>
</html>